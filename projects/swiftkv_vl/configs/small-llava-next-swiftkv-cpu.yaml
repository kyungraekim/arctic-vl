type: vl_swiftkv
code: ../train.py

# Training configuration
micro_batch_size: 1
epochs: 3
gradient_accumulation_steps: 4
temperature: 2.0
vision_loss_weight: 1.0
text_loss_weight: 1.0

# Model configuration  
model:
  type: vl_swiftkv
  # Use our small LlavaNext SwiftKV config
  name_or_path: projects.swiftkv.models.llava_next.create_small_llava_next_swiftkv_config
  num_key_value_layers: 2  # First 2 layers have full KV
  key_value_group_size: 1  # Simple 1:1 grouping
  # CPU-friendly settings (no flash attention)
  attn_implementation: eager

# Data configuration for VL training
data:
  type: vl_sft
  sources:
    # Use smaller datasets for CPU testing
    - HuggingFaceM4/VQAv2  # Vision QA dataset
    - lmsys/lmsys-chat-1m  # Text-only dataset for mixed training
  cache_dir: ./cache
  num_proc: 1  # Single process for CPU
  max_length: 512  # Shorter sequences for CPU
  max_images_per_sample: 2  # Limit images per sample
  image_processing_batch_size: 1
  pack_samples: false  # Disable packing for simplicity
  filter_samples: true
  pad_to: div_length
  div_length: 64  # Small divisible length for CPU
  mask_inputs: true

# CPU-optimized training settings
deepspeed:
  zero_optimization:
    stage: 0  # Disable ZeRO for CPU
  train_micro_batch_size_per_gpu: 1
  gradient_accumulation_steps: 4

# Logging
logger:
  level: INFO
  output_dir: "./logs"
  file_output_ranks: [0]

# Learning rate and optimization
scheduler:
  type: cosine
  warmup_ratio: 0.1
  
optimizer:
  type: adamw
  betas: [0.9, 0.999]
  weight_decay: 0.01
  lr: 0.0001  # Lower LR for stable training

# CPU-friendly checkpoint settings
checkpoint:
  - type: huggingface
    save_every_n_epochs: 1
    output_dir: ./checkpoints/small-llava-next-swiftkv
    save_end_of_training: true

# Tokenizer
tokenizer:
  type: huggingface
  name_or_path: microsoft/DialoGPT-small  # Small tokenizer for testing