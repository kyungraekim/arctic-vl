type: vl_swiftkv
code: ../train.py

# MPS-optimized training configuration
micro_batch_size: 2  # Larger batch size for MPS
epochs: 2
gradient_accumulation_steps: 2
temperature: 2.0
vision_loss_weight: 1.0
text_loss_weight: 1.0

# Model configuration  
model:
  type: vl_swiftkv_model
  # Use our small LlavaNext SwiftKV config
  name_or_path: projects.swiftkv.models.llava_next.create_small_llava_next_swiftkv_config
  num_key_value_layers: 2  # First 2 layers have full KV
  key_value_group_size: 1  # Simple 1:1 grouping
  # MPS-compatible settings
  attn_implementation: eager

# Data configuration for VL training
data:
  type: vl_sft
  sources:
    # Use smaller datasets for testing
    - HuggingFaceM4/VQAv2  # Vision QA dataset  
    - lmsys/lmsys-chat-1m  # Text-only dataset for mixed training
  cache_dir: ./cache
  num_proc: 2  # Can use more processes with MPS
  max_length: 256  # Moderate sequences for MPS
  max_images_per_sample: 2  # Limit images per sample
  image_processing_batch_size: 1
  pack_samples: false  # Disable packing for simplicity
  filter_samples: true
  pad_to: div_length
  div_length: 64  # Small divisible length
  mask_inputs: true

# MPS-optimized training settings  
deepspeed:
  zero_optimization:
    stage: 0  # Keep ZeRO disabled for MPS compatibility
  train_micro_batch_size_per_gpu: 2
  gradient_accumulation_steps: 2

# Logging
logger:
  level: INFO
  output_dir: "./logs"
  file_output_ranks: [0]

# Learning rate and optimization
scheduler:
  type: cosine
  warmup_ratio: 0.1
  
optimizer:
  type: adamw
  betas: [0.9, 0.999]
  weight_decay: 0.01
  lr: 0.0002  # Slightly higher LR for MPS

# MPS-friendly checkpoint settings
checkpoint:
  - type: huggingface
    save_every_n_epochs: 1
    output_dir: ./checkpoints/small-llava-next-swiftkv-mps
    save_end_of_training: true

# Tokenizer
tokenizer:
  type: huggingface
  name_or_path: microsoft/DialoGPT-small  # Small tokenizer for testing